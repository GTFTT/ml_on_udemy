{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:42:38.501506Z",
     "start_time": "2025-02-04T07:42:38.496509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from keras import datasets, layers, callbacks, models, optimizers, preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "id": "e7df59824fcdce79",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:43:07.839711Z",
     "start_time": "2025-02-04T07:43:07.834665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = [\n",
    "    'I like eggs and ham.',\n",
    "    'I love chocolate and bunnies.',\n",
    "    'I hate onions.'\n",
    "]"
   ],
   "id": "2f269cc384f04365",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:44:17.043732Z",
     "start_time": "2025-02-04T07:44:17.038108Z"
    }
   },
   "cell_type": "code",
   "source": "MAX_VOCAB_SIZE = 20_000",
   "id": "d566af8b15f2f604",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:46:37.097786Z",
     "start_time": "2025-02-04T07:46:37.089109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorization_layer = layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    # split='whitespace',\n",
    "    # output_mode='int',\n",
    ")"
   ],
   "id": "53f634233d134671",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:48:14.301544Z",
     "start_time": "2025-02-04T07:48:14.202012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For some reason they decided to use adapt instead of fit\n",
    "vectorization_layer.adapt(sentences)"
   ],
   "id": "2cc169a01d9d7cc8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:48:59.283622Z",
     "start_time": "2025-02-04T07:48:59.273852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we use the vectorization layer to convert the sentences to sequences of integers\n",
    "sequences = vectorization_layer(sentences)\n",
    "print(sequences)"
   ],
   "id": "ecfb1b968b8631ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  6  9  3  8]\n",
      " [ 2  5 10  3 11]\n",
      " [ 2  7  4  0  0]], shape=(3, 5), dtype=int64)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:50:49.603489Z",
     "start_time": "2025-02-04T07:50:49.593254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This list will contain the vocabulary in the order of the most common words and their indexes\n",
    "vectorization_layer.get_vocabulary()"
   ],
   "id": "f5fdaa919ad5d58f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " np.str_('i'),\n",
       " np.str_('and'),\n",
       " np.str_('onions'),\n",
       " np.str_('love'),\n",
       " np.str_('like'),\n",
       " np.str_('hate'),\n",
       " np.str_('ham'),\n",
       " np.str_('eggs'),\n",
       " np.str_('chocolate'),\n",
       " np.str_('bunnies')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:54:26.673014Z",
     "start_time": "2025-02-04T07:54:26.665685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Converting the sequences back to sentences\n",
    "word2index = {word: index for index, word in enumerate(vectorization_layer.get_vocabulary())}\n",
    "word2index"
   ],
   "id": "5f7b755466bde0b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " np.str_('i'): 2,\n",
       " np.str_('and'): 3,\n",
       " np.str_('onions'): 4,\n",
       " np.str_('love'): 5,\n",
       " np.str_('like'): 6,\n",
       " np.str_('hate'): 7,\n",
       " np.str_('ham'): 8,\n",
       " np.str_('eggs'): 9,\n",
       " np.str_('chocolate'): 10,\n",
       " np.str_('bunnies'): 11}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:57:04.539490Z",
     "start_time": "2025-02-04T07:57:04.500667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorization_layer_truncated = layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    output_sequence_length=3\n",
    ")\n",
    "\n",
    "vectorization_layer_truncated.adapt(sentences)\n",
    "\n",
    "sequences_truncated = vectorization_layer_truncated(sentences)\n",
    "print(sequences_truncated)"
   ],
   "id": "5b2fd5295bdbd2ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  6  9]\n",
      " [ 2  5 10]\n",
      " [ 2  7  4]], shape=(3, 3), dtype=int64)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:59:57.478426Z",
     "start_time": "2025-02-04T07:59:57.459202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ragged (no padding)\n",
    "vectorization_layer_ragged = layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    ragged=True\n",
    ")\n",
    "\n",
    "vectorization_layer_ragged.adapt(sentences)\n",
    "\n",
    "sequences_ragged = vectorization_layer_ragged(sentences)\n",
    "print(sequences_ragged)"
   ],
   "id": "375774d4aac116da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[2, 6, 9, 3, 8], [2, 5, 10, 3, 11], [2, 7, 4]]>\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
